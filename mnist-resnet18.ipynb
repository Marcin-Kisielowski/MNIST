{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-10T12:48:39.741946Z","iopub.execute_input":"2022-03-10T12:48:39.742204Z","iopub.status.idle":"2022-03-10T12:48:39.749956Z","shell.execute_reply.started":"2022-03-10T12:48:39.742175Z","shell.execute_reply":"2022-03-10T12:48:39.749031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\navailable_device = (torch.device('cuda') if torch.cuda.is_available() \n         else torch.device('cpu'))\nprint(\"Training on device \", available_device)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:48:42.501028Z","iopub.execute_input":"2022-03-10T12:48:42.502647Z","iopub.status.idle":"2022-03-10T12:48:42.509322Z","shell.execute_reply.started":"2022-03-10T12:48:42.502578Z","shell.execute_reply":"2022-03-10T12:48:42.508622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import models, transforms\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:52:54.639271Z","iopub.execute_input":"2022-03-10T12:52:54.639525Z","iopub.status.idle":"2022-03-10T12:52:54.643748Z","shell.execute_reply.started":"2022-03-10T12:52:54.639496Z","shell.execute_reply":"2022-03-10T12:52:54.642765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MNISTdigits(torch.utils.data.Dataset):\n  def __init__(self, file_name):\n        'Initialization'\n        self.all_data=pd.read_csv(file_name)\n        self.labels = torch.from_numpy(self.all_data[\"label\"].to_numpy())\n        data_points=self.all_data.iloc[:,1:].to_numpy()\n        self.data_tensor=torch.from_numpy(data_points/255).float()\n        self.transform=None\n  def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.labels)\n\n  def __getitem__(self, index):\n        'Generates one sample of data'\n        # Select sample\n       \n        X = self.data_tensor[index,:]\n        if self.transform:\n            X = self.transform(X)\n        y = self.labels[index]\n\n        return X, y","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:52:01.774195Z","iopub.execute_input":"2022-03-10T12:52:01.774467Z","iopub.status.idle":"2022-03-10T12:52:01.781975Z","shell.execute_reply.started":"2022-03-10T12:52:01.774436Z","shell.execute_reply":"2022-03-10T12:52:01.781085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MNISTdigits2d(MNISTdigits):\n     def __getitem__(self, index):\n        'Generates one sample of data'\n        # Select sample\n       \n        X = self.data_tensor[index,:].view(1,28,28)\n        if self.transform:\n            X = self.transform(X)\n        y = self.labels[index]\n\n        return X, y","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:52:04.221532Z","iopub.execute_input":"2022-03-10T12:52:04.222085Z","iopub.status.idle":"2022-03-10T12:52:04.227475Z","shell.execute_reply.started":"2022-03-10T12:52:04.222047Z","shell.execute_reply":"2022-03-10T12:52:04.226544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport time\n\nclass ModelTrainer:\n    def __init__(self,model,loss_function,optimizer):\n        self.model=model\n        self.loss_fn=loss_function\n        self.optimizer=optimizer\n        self.epochs=[]\n        self.losses=[]\n    def load_data(self,file_name,split_ratio_list):\n        self.data_set=MNISTdigits(file_name)\n        self.split_data(split_ratio_list)\n    def split_data(self,split_ratio_list):\n        split_ratio=torch.tensor(split_ratio_list)\n        lengths=split_ratio*(len(self.data_set)//100)\n        self.train_set,self.valid_set=torch.utils.data.random_split(self.data_set,lengths)\n    def normalize_train_data(self):\n        data_loader=torch.utils.data.DataLoader(self.train_set, batch_size=256, shuffle=False)\n        means=[]\n        stds=[]\n        for imgs,labels in data_loader:\n            means.append(torch.mean(imgs))\n            stds.append(torch.std(imgs))\n        self.mean = torch.mean(torch.tensor(means))\n        self.std = torch.mean(torch.tensor(stds))\n        self.data_set.transform = transforms.Normalize((self.mean),(self.std))\n    def train(self,n_epochs, batch_size=64):\n        self.model.train()\n        self.batch_size=batch_size\n        train_loader=torch.utils.data.DataLoader(self.train_set, batch_size=batch_size, shuffle=True)\n        initial_n_epochs=len(self.epochs)\n        start_time=time.time()\n        for epoch in range(initial_n_epochs+1,initial_n_epochs+n_epochs+1): #We add len(epochs) because we would like to continue where we started after running train() the next time\n            for imgs,labels in train_loader:\n                imgs=imgs.to(device=available_device)\n                labels=labels.to(device=available_device)\n                self.loss=self.loss_fn(self.model(imgs),labels)\n                self.optimizer.zero_grad()\n                self.loss.backward()\n                self.optimizer.step()\n            self.epochs.append(epoch) \n            self.losses.append(self.loss.detach().item())\n        end_time=time.time()\n        self.training_time=end_time-start_time\n    def get_training_time(self):\n        return self.training_time\n    def get_accuracy(self,dataset):\n        self.model.eval()\n        data_loader=torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n        correct_guesses=0\n        all_samples=0\n        with torch.no_grad():\n            for imgs, labels in data_loader:\n                imgs=imgs.to(device=available_device)\n                labels=labels.to(device=available_device)\n                _, predicted = torch.max(self.model(imgs), dim=1)\n                correct_guesses+=(predicted==labels).sum()\n                all_samples+=labels.shape[0]\n        return correct_guesses/all_samples\n    def get_train_valid_accuracy(self):\n        return self.get_accuracy(self.train_set), self.get_accuracy(self.valid_set)\n    def plot_loss_agains_n_epochs(self):\n        plt.rcParams['figure.figsize'] = [20, 10]\n        plt.plot(self.epochs,self.losses)\n    def save_parameters(self,file_name):\n        torch.save(self.model.state_dict(), file_name)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:51:27.788775Z","iopub.execute_input":"2022-03-10T12:51:27.789064Z","iopub.status.idle":"2022-03-10T12:51:27.837383Z","shell.execute_reply.started":"2022-03-10T12:51:27.789032Z","shell.execute_reply":"2022-03-10T12:51:27.836636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def analyze_model(model_trainer,batch_size,n_epochs):\n    model_trainer.train(n_epochs,batch_size)\n    model_trainer.plot_loss_agains_n_epochs()\n    train_accuracy, valid_accuracy = model_trainer.get_train_valid_accuracy()\n    return model_trainer.training_time,train_accuracy.item(),valid_accuracy.item()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:48:56.350309Z","iopub.execute_input":"2022-03-10T12:48:56.350562Z","iopub.status.idle":"2022-03-10T12:48:56.357235Z","shell.execute_reply.started":"2022-03-10T12:48:56.350534Z","shell.execute_reply":"2022-03-10T12:48:56.354727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResNetModelTrainer(ModelTrainer):\n    def __init__(self,learning_rate):\n        self.learning_rate=learning_rate\n        self.model=models.resnet18().to(device=available_device)\n        self.model.conv1=nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device=available_device)\n        self.model.fc=nn.Linear(512,10).to(device=available_device)\n        self.optimizer=torch.optim.SGD(self.model.parameters(),lr=learning_rate)\n        self.loss_fn=nn.CrossEntropyLoss()\n        self.epochs=[]\n        self.losses=[]\n    def load_data(self,file_name,split_ratio_list):\n        self.data_set=MNISTdigits2d(file_name)\n        self.split_data(split_ratio_list)\n        self.normalize_train_data()\n    def save_parameters(self):\n        self.parameters_file_name=\"./resnet_epochs_%d_batch_size_%d_lr_%f_val_accuracy_%f.pt\"%(len(self.epochs),self.batch_size,self.learning_rate,self.get_accuracy(self.valid_set))\n        super().save_parameters(self.parameters_file_name)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:53:00.467489Z","iopub.execute_input":"2022-03-10T12:53:00.467760Z","iopub.status.idle":"2022-03-10T12:53:00.478012Z","shell.execute_reply.started":"2022-03-10T12:53:00.467729Z","shell.execute_reply":"2022-03-10T12:53:00.476267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate=1.0e-01\n\nresnet=ResNetModelTrainer(learning_rate)\nsplit_ratio=[80,20]\nfile_name=\"../input/digit-recognizer/train.csv\"\nresnet.load_data(file_name,split_ratio)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:53:03.165257Z","iopub.execute_input":"2022-03-10T12:53:03.165504Z","iopub.status.idle":"2022-03-10T12:53:06.487315Z","shell.execute_reply.started":"2022-03-10T12:53:03.165477Z","shell.execute_reply":"2022-03-10T12:53:06.486351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results=analyze_model(resnet,16,10)\nprint(\"training time: %f \\t train accuracy: %f \\t valid accuracy: %f \"%results)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:53:11.508329Z","iopub.execute_input":"2022-03-10T12:53:11.508577Z","iopub.status.idle":"2022-03-10T12:57:21.315652Z","shell.execute_reply.started":"2022-03-10T12:53:11.508551Z","shell.execute_reply":"2022-03-10T12:57:21.314996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate=1.0e-03\n\nresnet.optimizer=torch.optim.SGD(resnet.model.parameters(),lr=learning_rate)\nresults=analyze_model(resnet,16,10)\nprint(\"training time: %f \\t train accuracy: %f \\t valid accuracy: %f \"%results)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T13:07:11.819155Z","iopub.execute_input":"2022-03-10T13:07:11.819436Z","iopub.status.idle":"2022-03-10T13:11:16.637966Z","shell.execute_reply.started":"2022-03-10T13:07:11.819405Z","shell.execute_reply":"2022-03-10T13:11:16.637278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate=1.0e-06\n\nresnet.optimizer=torch.optim.SGD(resnet.model.parameters(),lr=learning_rate)\nresults=analyze_model(resnet,16,10)\nprint(\"training time: %f \\t train accuracy: %f \\t valid accuracy: %f \"%results)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T13:13:23.898572Z","iopub.execute_input":"2022-03-10T13:13:23.899084Z","iopub.status.idle":"2022-03-10T13:17:30.877943Z","shell.execute_reply.started":"2022-03-10T13:13:23.899048Z","shell.execute_reply":"2022-03-10T13:17:30.877261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MNISTdigits2dTest(torch.utils.data.Dataset):\n  def __init__(self, file_name):\n        'Initialization'\n        test_data=pd.read_csv(file_name)\n        data_points=test_data.to_numpy()\n        self.data_tensor=torch.from_numpy(data_points/255).float()\n        self.data_tensor=self.data_tensor.view(-1,1,28,28)\n  def __len__(self):\n        'Denotes the total number of samples'\n        return self.data_tensor.shape[0]\n\n  def __getitem__(self, index):\n        'Generates one sample of data'\n        # Select sample\n       \n        X = self.data_tensor[index,:]\n        if self.transform:\n            X = self.transform(X)\n\n        return index+1, X","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:47:54.313142Z","iopub.status.idle":"2022-03-10T12:47:54.313873Z","shell.execute_reply.started":"2022-03-10T12:47:54.313646Z","shell.execute_reply":"2022-03-10T12:47:54.313669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission_file(model,out_file_name):\n    model.eval()\n    data_set=MNISTdigits2dTest('../input/digit-recognizer/test.csv')\n    data_loader=torch.utils.data.DataLoader(data_set, batch_size=64, shuffle=False)\n    indices=[]\n    predictions=[]\n    with torch.no_grad():\n        for batch_indices,batch_imgs in data_loader:\n            batch_imgs=batch_imgs.to(device=available_device)\n            batch_indices=batch_indices.to(device=available_device)\n            _, batch_predictions = torch.max(model(batch_imgs), dim=1)\n            indices.append(batch_indices)\n            predictions.append(batch_predictions)\n    tensor_indices=torch.cat(tuple(indices),dim=0)\n    tensor_predictions=torch.cat(tuple(predictions),dim=0)\n    result_data_frame=pd.DataFrame({'ImageId':tensor_indices.cpu().numpy(), 'Label':tensor_predictions.cpu().numpy()})\n    result_data_frame.set_index('ImageId')\n    result_data_frame.to_csv(out_file_name,index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:47:54.314956Z","iopub.status.idle":"2022-03-10T12:47:54.315710Z","shell.execute_reply.started":"2022-03-10T12:47:54.315460Z","shell.execute_reply":"2022-03-10T12:47:54.315485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_submission_file(resnet.model,'./submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T12:47:54.316855Z","iopub.status.idle":"2022-03-10T12:47:54.317580Z","shell.execute_reply.started":"2022-03-10T12:47:54.317348Z","shell.execute_reply":"2022-03-10T12:47:54.317372Z"},"trusted":true},"execution_count":null,"outputs":[]}]}